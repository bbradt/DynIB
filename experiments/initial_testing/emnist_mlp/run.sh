sbatch -J EM-DynIB-k0 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf0 --kf 10 --k 0 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k1 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf1 --kf 10 --k 1 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k2 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf2 --kf 10 --k 2 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k3 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf3 --kf 10 --k 3 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k4 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf4 --kf 10 --k 4 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k5 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf5 --kf 10 --k 5 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k6 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf6 --kf 10 --k 6 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k7 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf7 --kf 10 --k 7 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k8 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf8 --kf 10 --k 8 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
sbatch -J EM-DynIB-k9 experiments/runner.sh --logdir /data/users2/bbaker/projects/dynib/results/initial_emnist_mlp/kf9 --kf 10 --k 9 --criterion CrossEntropyLoss --scheduler ExponentialLR --optim Adam --num_epochs  100 --lr 1e-6 --model_kwargs '{"hidden_layers":[512,256,128,64,32,16]}' --model_args '[dataset[0][0].shape,num_classes]' --model mlp --dataset fmnist --seed 0 --batch_size 128
